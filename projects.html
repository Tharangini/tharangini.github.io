<script src="http://code.jquery.com/jquery-2.1.4.min.js"></script>
<script>
    $(function(){
        $('a').each(function(){
            if ($(this).prop('href') == window.location.href) {
                $(this).addClass('active'); $(this).parents('navbar').addClass('active');
            }
        });
    });
</script>

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>About | Tharangini Sankarnarayanan</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="About" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://tharangini.github.io/" />
<meta property="og:url" content="https://tharangini.github.io/" />
<meta property="og:site_name" content="Tharangini Sankarnarayanan" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="About" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"About","name":"Tharangini Sankarnarayanan","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://tharangini.github.io/assets/img/profile.jpg"}},"url":"https://tharangini.github.io/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="assets/css/style.css?v=19c56430dae9f068c39678ce660a302362d519ab">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <link rel="shortcut icon" type="image/x-icon" href="assets/img/icon-girl.jpg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;1,300;1,400;1,600&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">

    <style type="text/css">
  </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-144023591-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-144023591-1');
    </script>
</head>
  <body style="background-image:url('assets/img/neurons3.png')">
    <div class="wrapper">
      <div class="header">
        <img src="assets/img/profiles.jpeg" alt="Tharangini Sankarnarayanan"/>
        <h1> Tharangini Sankarnarayanan </h1>
        <p>
          ts4180 [at] nyu.edu
        </p>
        <p> 
          [<a href=https://scholar.google.com/citations?hl=en&user=ElPFB8YAAAAJ>Google Scholar</a>]
          [<a href=https://github.com/tharangini>Github</a>]
          [<a href=assets/doc/CV.pdf>CV</a>]
        </p> 
        <div class="navbar">
          <a class="active" href="./index.html"> About </a> <a href="./projects.html"> Projects </a> <a href="./teaching.html"> Teaching </a>
        </div>
      </div>
      <div class="content">
      	

       
<h2>Papers</h2>
<br>
<table> 
  
  <tr>
    <td style="vertical-align: text-top;">
      <img src="assets/img/object_detection.gif" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Training AI to recognize distances, objects, and requests of interest to the blind community
        </h3>
        IEEE Proceedings and PubMed, 2023</a>
        Authors </a>
  <br>
  <br>
<p>Presenting at IEEE EMBC 2023, Sydney. Under the supervision of <a href="https://med.nyu.edu/faculty/kevin-c-chan">Kevin C. Chan</a>, the project focuses on training an object detection model to recognize items relevant to blind people in their daily lives. The pipeline begins with the identification of thirty-five objects of interest to the blind community via surveys, questionnaires, and Microsoft ORBIT research. The pipeline involves training a 'You Only Look Once' (YOLO) bounding box object detection model on a resampled version of the publicly available Google Open Images V6 dataset. For end users, the model incorporates portable assistive technologies. The work is a submission to the 45th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.</p>
  <br>
</td>
  </tr>
 
  
  <tr>
    <td style="vertical-align: text-top;">
      <img src="assets/img/image-inpainting.png" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Realistic Face Rendering for 3D Mixed Reality Experience</h3>
      FIB - Facultat d'Informàtica de Barcelona
      (2019-07-1)
  <br>
  <br>
  <p>I designed and conceptualized the prototype of a deep learning pipeline for time-series analysis of video communication data to perform an in-depth research project. Devised a bounding box to detect the Virtual Reality headsets in the video captures and creates a realistic model of the user’s face and use it to replace the part covered by the headset, enabling mixed reality experience. The accuracy of the resulting prototype was 79%. The project was advised by <a href="https://imatge.upc.edu/web/people/josep-ramon-morros">Josep Ramon Morros</a>, <a href="https://imatge.upc.edu/web/people/javier-ruiz-hidalgo">Javier Ruiz Hidalgo</a> and <a href="https://www.cs.upc.edu/~castell/">Núria Castell Ariño</a>.</p>
  <br>
        
  <tr>
    <td style="vertical-align: text-top;">
      <img src="assets/img/object_detection.gif" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Training AI to recognize distances, objects, and requests of interest to the blind community
        </h3>
        IEEE Proceedings and PubMed, 2023</a>
        Authors </a>
  <br>
  <br>
<p>Presenting at IEEE EMBC 2023, Sydney. Under the supervision of <a href="https://med.nyu.edu/faculty/kevin-c-chan">Kevin C. Chan</a>, the project focuses on training an object detection model to recognize items relevant to blind people in their daily lives. The pipeline begins with the identification of thirty-five objects of interest to the blind community via surveys, questionnaires, and Microsoft ORBIT research. The pipeline involves training a 'You Only Look Once' (YOLO) bounding box object detection model on a resampled version of the publicly available Google Open Images V6 dataset. For end users, the model incorporates portable assistive technologies. The work is a submission to the 45th Annual International Conference of the IEEE Engineering in Medicine and Biology Society.</p>
  <br>
</td>
  </tr>
  
</td>
  </tr>
</table>

<h2>Projects</h2>
<br>
<table>
    
    <td style="vertical-align: text-top;">
      <img src="assets/img/gpt-2-output.gif" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Language Generation, Modeling and Bias Evaluation on BOLD Dataset</h3>
      </a>
      (2022-12-24)

  <br>
  <br>
<p> The project evaluates the potential of Language Models (LMs) to generate potentially dangerous biases resulting from stereotypes that spread hostile generalizations. Text is generated using prompts of the Bias in the Open-Ended Language Generation Dataset (BOLD). Evaluation metrics of toxicity, sentiment, and regard are implemented on the generated texts to evaluate the implicit bias in pre-trained language models. The project researches the different degrees of biases across social groups in society between GPT-2, CTRL, XL-Net, GPT-Neo, and OPT.</p>
  <br>
</td>
  </tr>
  
  
  
  

  
  
  <tr>
    <td style="vertical-align: text-top;">
      <img src="assets/img/job_board.png" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Job Board for Incarcerators</h3>
      Marron Institute of Urban Management: Litmus Project
      (2022-08-28)

  <br>
  <br>
  <p>I designed a tool under the supervision of <a href="https://marroninstitute.nyu.edu/people/angela-hawken1">Angela Hawken</a> to connect employers committed to hiring people with criminal records with qualified applicants who are releasing from the Illinois Department of Corrections, collaborating with a group of 10 end users to create features across the software using Python, Flask, and CSS as part of the <a href="https://marroninstitute.nyu.edu/blog/litmus-consults-with-illinois-corrections-and-data-project-partners-and-funders">Litmus Project</a>. The prototype of the tool was presented to incarcerators at Kewanee Life Skills Re-entry Center and the Director of Illinois Department of Corrections and the tool will be launched in August 2023.</p>

  <br>
</td>
  </tr>

<tr>
    <td style="vertical-align: text-top;">
      <img src="assets/img/typicality-ratings.png" alt="project image" style="width:auto; height: auto; padding-top: 12%;"/>
    </td>
    <td style="padding-top: 3%; padding-bottom: 5%; padding-left: 8%; width:75%;vertical-align:top">
      <h3>Comparing Typicality Ratings between Human and Model Representations for Images</h3>
      DS-GA 1009: Computational Cognitive Modelling
      (2022-05-10)

  <br>
  <br>
  <p>Supervised by <a href="https://cims.nyu.edu/~brenden/">Brendan Lake</a>, the project rrevolves around an extension to compare the typicality ratings of each image of the domain by Humans, Convolutional Networks, and Vision Transformers. The domains under consideration were rose, mushroom, dinosaur, fish and couch. I experimented with Convolutional Networks of GoogleNet and ResNet and Visual Transformers. Visual Transformers outperform Convolutional Networks in typicality prediction and model the human mind in 3 out of the 5 domains using PyTorch</p>

  <br>
</td>
  </tr>
</div>
  </body>
</html>
